{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense , Dropout , Lambda, Flatten\n",
    "from keras.optimizers import SGD\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape: (42000, 785)\n",
      "test shape: (28000, 784)\n",
      "train data head:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>pixel0</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel774</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  \\\n",
       "0      1       0       0       0       0       0       0       0       0   \n",
       "1      0       0       0       0       0       0       0       0       0   \n",
       "2      1       0       0       0       0       0       0       0       0   \n",
       "3      4       0       0       0       0       0       0       0       0   \n",
       "4      0       0       0       0       0       0       0       0       0   \n",
       "\n",
       "   pixel8    ...     pixel774  pixel775  pixel776  pixel777  pixel778  \\\n",
       "0       0    ...            0         0         0         0         0   \n",
       "1       0    ...            0         0         0         0         0   \n",
       "2       0    ...            0         0         0         0         0   \n",
       "3       0    ...            0         0         0         0         0   \n",
       "4       0    ...            0         0         0         0         0   \n",
       "\n",
       "   pixel779  pixel780  pixel781  pixel782  pixel783  \n",
       "0         0         0         0         0         0  \n",
       "1         0         0         0         0         0  \n",
       "2         0         0         0         0         0  \n",
       "3         0         0         0         0         0  \n",
       "4         0         0         0         0         0  \n",
       "\n",
       "[5 rows x 785 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_data = pd.read_csv('../input/train.csv')\n",
    "te_data = pd.read_csv('../input/test.csv')\n",
    "print('train shape: {}'.format(tr_data.shape))\n",
    "print('test shape: {}'.format(te_data.shape))\n",
    "print('train data head:')\n",
    "tr_data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "as we can see the data has 785 columns which are 28*28 pixels +1 labels column (which will only appear in the train data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test data head:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pixel0</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>pixel9</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel774</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 784 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  pixel8  \\\n",
       "0       0       0       0       0       0       0       0       0       0   \n",
       "1       0       0       0       0       0       0       0       0       0   \n",
       "2       0       0       0       0       0       0       0       0       0   \n",
       "3       0       0       0       0       0       0       0       0       0   \n",
       "4       0       0       0       0       0       0       0       0       0   \n",
       "\n",
       "   pixel9    ...     pixel774  pixel775  pixel776  pixel777  pixel778  \\\n",
       "0       0    ...            0         0         0         0         0   \n",
       "1       0    ...            0         0         0         0         0   \n",
       "2       0    ...            0         0         0         0         0   \n",
       "3       0    ...            0         0         0         0         0   \n",
       "4       0    ...            0         0         0         0         0   \n",
       "\n",
       "   pixel779  pixel780  pixel781  pixel782  pixel783  \n",
       "0         0         0         0         0         0  \n",
       "1         0         0         0         0         0  \n",
       "2         0         0         0         0         0  \n",
       "3         0         0         0         0         0  \n",
       "4         0         0         0         0         0  \n",
       "\n",
       "[5 rows x 784 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('test data head:')\n",
    "te_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "as usual lets separate our training data from the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = tr_data.iloc[:,1:].values.astype('float32') # all pixel values\n",
    "y_train = tr_data.iloc[:,0].values.astype('int32') # only labels i.e targets digits\n",
    "X_test = te_data.values.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:1.69801+0.0032127\ttest-mlogloss:1.71193+0.00702032\n",
      "[2]\ttrain-mlogloss:1.1779+0.00332384\ttest-mlogloss:1.20553+0.00647845\n",
      "[4]\ttrain-mlogloss:0.883198+0.00420416\ttest-mlogloss:0.917629+0.00597328\n",
      "[6]\ttrain-mlogloss:0.688887+0.00174742\ttest-mlogloss:0.7287+0.00747862\n",
      "[8]\ttrain-mlogloss:0.554026+0.00224428\ttest-mlogloss:0.598385+0.00718724\n",
      "[10]\ttrain-mlogloss:0.4555+0.00216103\ttest-mlogloss:0.503589+0.00699719\n",
      "[12]\ttrain-mlogloss:0.38099+0.00226145\ttest-mlogloss:0.432329+0.00688715\n",
      "[14]\ttrain-mlogloss:0.323871+0.00305608\ttest-mlogloss:0.378545+0.00634408\n",
      "[16]\ttrain-mlogloss:0.278592+0.00306677\ttest-mlogloss:0.335212+0.0061126\n",
      "[18]\ttrain-mlogloss:0.24304+0.00233473\ttest-mlogloss:0.302117+0.00646911\n",
      "[20]\ttrain-mlogloss:0.214098+0.00200352\ttest-mlogloss:0.275589+0.00671574\n",
      "[22]\ttrain-mlogloss:0.190429+0.00209449\ttest-mlogloss:0.253872+0.00635668\n",
      "[24]\ttrain-mlogloss:0.170813+0.00181613\ttest-mlogloss:0.236119+0.00625868\n",
      "[26]\ttrain-mlogloss:0.153368+0.00155479\ttest-mlogloss:0.220444+0.00592237\n",
      "[28]\ttrain-mlogloss:0.138864+0.00136607\ttest-mlogloss:0.20737+0.00599197\n",
      "CPU times: user 25min 30s, sys: 3.35 s, total: 25min 33s\n",
      "Wall time: 6min 35s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import xgboost as xgb\n",
    "params = {'objective':'multi:softprob',\n",
    "                    'learning_rate':0.2,\n",
    "                    'subsample':0.8,\n",
    "                    'colsample_bytree':0.9,\n",
    "                    'colsample_bylevel':0.7,\n",
    "                    'max_depth':5,\n",
    "                    'nthread':4,\n",
    "                    'eval_metric':'mlogloss',\n",
    "                    'n_estimators':100,\n",
    "                    'num_class':10,\n",
    "                    'seed':1234}\n",
    "bst_cv = xgb.cv(params=params,dtrain=xgb.DMatrix(X_train,label=y_train),verbose_eval=2,\n",
    "                nfold=5,early_stopping_rounds=20,num_boost_round=30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1h 50s, sys: 2.41 s, total: 1h 52s\n",
      "Wall time: 16min 8s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import cross_val_score,KFold\n",
    "knn4 = KNeighborsClassifier(n_jobs=-1,n_neighbors=4)\n",
    "print(cross_val_score(cv=5,estimator=knn4,X=X_train,y=y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found array with dim 4. Estimator expected <= 2.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-71a1cc68eac1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m                     seed=1234)\n\u001b[1;32m     12\u001b[0m \u001b[0mrfecv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRFECV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgbc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'neg_log_loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mrfecv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Optimal number of features :{}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrfecv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_features_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'rfecv rankings: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrfecv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mranking_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/sklearn/feature_selection/rfe.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    390\u001b[0m             regression).\n\u001b[1;32m    391\u001b[0m         \"\"\"\n\u001b[0;32m--> 392\u001b[0;31m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"csr\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    393\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m         \u001b[0;31m# Initialization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    519\u001b[0m     X = check_array(X, accept_sparse, dtype, order, copy, force_all_finite,\n\u001b[1;32m    520\u001b[0m                     \u001b[0mensure_2d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_nd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_min_samples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m                     ensure_min_features, warn_on_dtype, estimator)\n\u001b[0m\u001b[1;32m    522\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m         y = check_array(y, 'csr', force_all_finite=True, ensure_2d=False,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    403\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mallow_nd\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m             raise ValueError(\"Found array with dim %d. %s expected <= 2.\"\n\u001b[0;32m--> 405\u001b[0;31m                              % (array.ndim, estimator_name))\n\u001b[0m\u001b[1;32m    406\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m             \u001b[0m_assert_all_finite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found array with dim 4. Estimator expected <= 2."
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.feature_selection import RFECV\n",
    "gbc = XGBClassifier(objective='multi:softprob',\n",
    "                    learning_rate=0.2,\n",
    "                    subsample=0.9,\n",
    "                    colsample_bytree=1,\n",
    "                    colsample_bylevel=1,\n",
    "                    max_depth=3,\n",
    "                    nthread=4,\n",
    "                    n_estimators=10,\n",
    "                    seed=1234)\n",
    "rfecv = RFECV(estimator=gbc, step=5, cv=5, scoring='neg_log_loss')\n",
    "rfecv.fit(X_train, y_train)\n",
    "print(\"Optimal number of features :{}\".format(rfecv.n_features_))\n",
    "print('rfecv rankings: {}'.format(rfecv.ranking_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now lets try to use only these features for prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVEAAAEICAYAAADm98d9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f9d98785400>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_train = X_train.reshape(X_train.shape[0], 28, 28)\n",
    "\n",
    "for i in range(0, 9):\n",
    "    plt.subplot(330 + (i+1))\n",
    "    plt.imshow(X_train[i], cmap=plt.get_cmap('Greens'))\n",
    "    plt.title(y_train[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# add code here to use only selected features\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now lets see what NN can get on this data, \n",
    "but first we'll have to preprocess it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_mean = X_train.mean()\n",
    "train_std = X_train.std()\n",
    "\n",
    "X_train = (X_train-train_mean)/train_std\n",
    "X_text = (X_test-train_mean)/train_std # pay attention that we normalize by train data and not by test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "as we learned, in classification tasks the output of each node in the last layer of the network represent the probability\n",
    "for each number - this means that we have to one-hot-encode each of the classes so that we will be able to compare the probability for each of the digits.\n",
    "\n",
    "luckily, keras can easily do that for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.utils.np_utils import to_categorical\n",
    "OHE_y_train = to_categorical(y_train)\n",
    "num_classes = OHE_y_train.shape[1]\n",
    "X_train = X_train.reshape(X_train.shape[0], 28, 28,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_7 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 10)                7850      \n",
      "=================================================================\n",
      "Total params: 7,850\n",
      "Trainable params: 7,850\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 33600 samples, validate on 8400 samples\n",
      "Epoch 1/10\n",
      "33600/33600 [==============================] - 5s - loss: 0.4254 - acc: 0.8734 - val_loss: 0.3148 - val_acc: 0.9082\n",
      "Epoch 2/10\n",
      "33600/33600 [==============================] - 5s - loss: 0.3070 - acc: 0.9126 - val_loss: 0.3208 - val_acc: 0.9055\n",
      "Epoch 3/10\n",
      "33600/33600 [==============================] - 5s - loss: 0.2932 - acc: 0.9171 - val_loss: 0.2999 - val_acc: 0.9161\n",
      "Epoch 4/10\n",
      "33600/33600 [==============================] - 5s - loss: 0.2843 - acc: 0.9203 - val_loss: 0.2902 - val_acc: 0.9212\n",
      "Epoch 5/10\n",
      "33600/33600 [==============================] - 5s - loss: 0.2778 - acc: 0.9233 - val_loss: 0.2950 - val_acc: 0.9204\n",
      "Epoch 6/10\n",
      "33600/33600 [==============================] - 5s - loss: 0.2728 - acc: 0.9262 - val_loss: 0.3124 - val_acc: 0.9163\n",
      "Epoch 7/10\n",
      "33600/33600 [==============================] - 5s - loss: 0.2709 - acc: 0.9245 - val_loss: 0.3111 - val_acc: 0.9177\n",
      "Epoch 8/10\n",
      "33600/33600 [==============================] - 5s - loss: 0.2667 - acc: 0.9263 - val_loss: 0.3114 - val_acc: 0.9175\n",
      "Epoch 9/10\n",
      "33600/33600 [==============================] - 5s - loss: 0.2650 - acc: 0.9268 - val_loss: 0.3018 - val_acc: 0.9200\n",
      "Epoch 10/10\n",
      "33600/33600 [==============================] - 5s - loss: 0.2635 - acc: 0.9289 - val_loss: 0.3051 - val_acc: 0.9187\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f9d74fdff98>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model= Sequential()\n",
    "model.add(Flatten(input_shape=(28,28,1)))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "from keras.optimizers import RMSprop\n",
    "model.compile(optimizer=RMSprop(lr=0.001),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "\n",
    "model.fit(X_train,OHE_y_train,validation_split=0.2,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_21 (Flatten)         (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_43 (Dense)             (None, 10)                7850      \n",
      "=================================================================\n",
      "Total params: 7,850\n",
      "Trainable params: 7,850\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 33600 samples, validate on 8400 samples\n",
      "Epoch 1/10\n",
      "33600/33600 [==============================] - 5s - loss: 0.6076 - acc: 0.8153 - val_loss: 0.3653 - val_acc: 0.8938\n",
      "Epoch 2/10\n",
      "33600/33600 [==============================] - 5s - loss: 0.3409 - acc: 0.9008 - val_loss: 0.3213 - val_acc: 0.9088\n",
      "Epoch 3/10\n",
      "33600/33600 [==============================] - 5s - loss: 0.3111 - acc: 0.9092 - val_loss: 0.3096 - val_acc: 0.9107\n",
      "Epoch 4/10\n",
      "33600/33600 [==============================] - 5s - loss: 0.2965 - acc: 0.9147 - val_loss: 0.2945 - val_acc: 0.9171\n",
      "Epoch 5/10\n",
      "33600/33600 [==============================] - 5s - loss: 0.2867 - acc: 0.9179 - val_loss: 0.2942 - val_acc: 0.9154\n",
      "Epoch 6/10\n",
      "33600/33600 [==============================] - 5s - loss: 0.2797 - acc: 0.9204 - val_loss: 0.2901 - val_acc: 0.9182\n",
      "Epoch 7/10\n",
      "33600/33600 [==============================] - 5s - loss: 0.2745 - acc: 0.9217 - val_loss: 0.2898 - val_acc: 0.9169\n",
      "Epoch 8/10\n",
      "33600/33600 [==============================] - 5s - loss: 0.2697 - acc: 0.9227 - val_loss: 0.2871 - val_acc: 0.9182\n",
      "Epoch 9/10\n",
      "33600/33600 [==============================] - 5s - loss: 0.2666 - acc: 0.9244 - val_loss: 0.2799 - val_acc: 0.9219\n",
      "Epoch 10/10\n",
      "33600/33600 [==============================] - 5s - loss: 0.2638 - acc: 0.9249 - val_loss: 0.2821 - val_acc: 0.9224\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f9d643d6f98>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model= Sequential()\n",
    "model.add(Flatten(input_shape=(28,28,1)))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "from keras.optimizers import Adadelta\n",
    "model.compile(optimizer=Adadelta(),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "\n",
    "model.fit(X_train,OHE_y_train,validation_split=0.2,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_23 (Flatten)         (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_46 (Dense)             (None, 10)                7850      \n",
      "_________________________________________________________________\n",
      "dense_47 (Dense)             (None, 10)                110       \n",
      "=================================================================\n",
      "Total params: 7,960\n",
      "Trainable params: 7,960\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 33600 samples, validate on 8400 samples\n",
      "Epoch 1/10\n",
      "33600/33600 [==============================] - 6s - loss: 0.5244 - acc: 0.8434 - val_loss: 0.3413 - val_acc: 0.9029\n",
      "Epoch 2/10\n",
      "33600/33600 [==============================] - 5s - loss: 0.3217 - acc: 0.9062 - val_loss: 0.3216 - val_acc: 0.9055\n",
      "Epoch 3/10\n",
      "33600/33600 [==============================] - 6s - loss: 0.2917 - acc: 0.9158 - val_loss: 0.3033 - val_acc: 0.9135\n",
      "Epoch 4/10\n",
      "33600/33600 [==============================] - 5s - loss: 0.2731 - acc: 0.9207 - val_loss: 0.2855 - val_acc: 0.9176\n",
      "Epoch 5/10\n",
      "33600/33600 [==============================] - 5s - loss: 0.2600 - acc: 0.9250 - val_loss: 0.2771 - val_acc: 0.9213\n",
      "Epoch 6/10\n",
      "33600/33600 [==============================] - 5s - loss: 0.2489 - acc: 0.9281 - val_loss: 0.2839 - val_acc: 0.9198\n",
      "Epoch 7/10\n",
      "33600/33600 [==============================] - 5s - loss: 0.2391 - acc: 0.9313 - val_loss: 0.2762 - val_acc: 0.9207\n",
      "Epoch 8/10\n",
      "33600/33600 [==============================] - 5s - loss: 0.2347 - acc: 0.9324 - val_loss: 0.2739 - val_acc: 0.9245\n",
      "Epoch 9/10\n",
      "33600/33600 [==============================] - 5s - loss: 0.2299 - acc: 0.9328 - val_loss: 0.2725 - val_acc: 0.9260\n",
      "Epoch 10/10\n",
      "33600/33600 [==============================] - 5s - loss: 0.2258 - acc: 0.9354 - val_loss: 0.2682 - val_acc: 0.9246\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f9d5fff0c18>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model= Sequential()\n",
    "model.add(Flatten(input_shape=(28,28,1)))\n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "from keras.optimizers import RMSprop\n",
    "model.compile(optimizer=RMSprop(lr=0.001),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "\n",
    "model.fit(X_train,OHE_y_train,validation_split=0.2,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_9 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 200)               157000    \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 10)                2010      \n",
      "=================================================================\n",
      "Total params: 159,010\n",
      "Trainable params: 159,010\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 33600 samples, validate on 8400 samples\n",
      "Epoch 1/10\n",
      "33600/33600 [==============================] - 5s - loss: 0.2623 - acc: 0.9215 - val_loss: 0.1536 - val_acc: 0.9531\n",
      "Epoch 2/10\n",
      "33600/33600 [==============================] - 5s - loss: 0.1174 - acc: 0.9652 - val_loss: 0.1241 - val_acc: 0.9654\n",
      "Epoch 3/10\n",
      "33600/33600 [==============================] - 5s - loss: 0.0824 - acc: 0.9746 - val_loss: 0.1223 - val_acc: 0.9683\n",
      "Epoch 4/10\n",
      "33600/33600 [==============================] - 5s - loss: 0.0637 - acc: 0.9808 - val_loss: 0.1200 - val_acc: 0.9708\n",
      "Epoch 5/10\n",
      "33600/33600 [==============================] - 5s - loss: 0.0469 - acc: 0.9864 - val_loss: 0.1283 - val_acc: 0.9724\n",
      "Epoch 6/10\n",
      "33600/33600 [==============================] - 5s - loss: 0.0397 - acc: 0.9883 - val_loss: 0.1289 - val_acc: 0.9705\n",
      "Epoch 7/10\n",
      "33600/33600 [==============================] - 5s - loss: 0.0326 - acc: 0.9899 - val_loss: 0.1401 - val_acc: 0.9712\n",
      "Epoch 8/10\n",
      "33600/33600 [==============================] - 5s - loss: 0.0255 - acc: 0.9923 - val_loss: 0.1285 - val_acc: 0.9758\n",
      "Epoch 9/10\n",
      "33600/33600 [==============================] - 5s - loss: 0.0205 - acc: 0.9937 - val_loss: 0.1322 - val_acc: 0.9743\n",
      "Epoch 10/10\n",
      "33600/33600 [==============================] - 5s - loss: 0.0164 - acc: 0.9950 - val_loss: 0.1582 - val_acc: 0.9718\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f9d6973cf60>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model= Sequential()\n",
    "model.add(Flatten(input_shape=(28,28,1)))\n",
    "model.add(Dense(200, activation='relu'))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "from keras.optimizers import RMSprop\n",
    "model.compile(optimizer=RMSprop(lr=0.001),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "\n",
    "model.fit(X_train,OHE_y_train,validation_split=0.2,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_15 (Flatten)         (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 300)               235500    \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 10)                3010      \n",
      "=================================================================\n",
      "Total params: 238,510\n",
      "Trainable params: 238,510\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 33600 samples, validate on 8400 samples\n",
      "Epoch 1/10\n",
      "33600/33600 [==============================] - 6s - loss: 0.2908 - acc: 0.9119 - val_loss: 0.1439 - val_acc: 0.9565\n",
      "Epoch 2/10\n",
      "33600/33600 [==============================] - 6s - loss: 0.1503 - acc: 0.9570 - val_loss: 0.1350 - val_acc: 0.9636\n",
      "Epoch 3/10\n",
      "33600/33600 [==============================] - 5s - loss: 0.1162 - acc: 0.9668 - val_loss: 0.1240 - val_acc: 0.9671\n",
      "Epoch 4/10\n",
      "33600/33600 [==============================] - 5s - loss: 0.1004 - acc: 0.9727 - val_loss: 0.1088 - val_acc: 0.9739\n",
      "Epoch 5/10\n",
      "33600/33600 [==============================] - 5s - loss: 0.0851 - acc: 0.9783 - val_loss: 0.1193 - val_acc: 0.9742\n",
      "Epoch 6/10\n",
      "33600/33600 [==============================] - 5s - loss: 0.0737 - acc: 0.9799 - val_loss: 0.1324 - val_acc: 0.9721\n",
      "Epoch 7/10\n",
      "33600/33600 [==============================] - 6s - loss: 0.0661 - acc: 0.9821 - val_loss: 0.1374 - val_acc: 0.9737\n",
      "Epoch 8/10\n",
      "33600/33600 [==============================] - 6s - loss: 0.0608 - acc: 0.9841 - val_loss: 0.1473 - val_acc: 0.9727\n",
      "Epoch 9/10\n",
      "33600/33600 [==============================] - 5s - loss: 0.0569 - acc: 0.9857 - val_loss: 0.1312 - val_acc: 0.9761\n",
      "Epoch 10/10\n",
      "33600/33600 [==============================] - 5s - loss: 0.0523 - acc: 0.9865 - val_loss: 0.1408 - val_acc: 0.9773\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f9d65a1e390>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model= Sequential()\n",
    "model.add(Flatten(input_shape=(28,28,1)))\n",
    "model.add(Dense(300, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "from keras.optimizers import RMSprop\n",
    "model.compile(optimizer=RMSprop(lr=0.001),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "\n",
    "model.fit(X_train,OHE_y_train,validation_split=0.2,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_20 (Flatten)         (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_41 (Dense)             (None, 300)               235500    \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_42 (Dense)             (None, 10)                3010      \n",
      "=================================================================\n",
      "Total params: 238,510\n",
      "Trainable params: 238,510\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 33600 samples, validate on 8400 samples\n",
      "Epoch 1/10\n",
      "33600/33600 [==============================] - 6s - loss: 0.3639 - acc: 0.8891 - val_loss: 0.1885 - val_acc: 0.9439\n",
      "Epoch 2/10\n",
      "33600/33600 [==============================] - 6s - loss: 0.1741 - acc: 0.9482 - val_loss: 0.1378 - val_acc: 0.9586\n",
      "Epoch 3/10\n",
      "33600/33600 [==============================] - 6s - loss: 0.1314 - acc: 0.9620 - val_loss: 0.1220 - val_acc: 0.9643\n",
      "Epoch 4/10\n",
      "33600/33600 [==============================] - 6s - loss: 0.1036 - acc: 0.9695 - val_loss: 0.1092 - val_acc: 0.9680\n",
      "Epoch 5/10\n",
      "33600/33600 [==============================] - 6s - loss: 0.0864 - acc: 0.9753 - val_loss: 0.1083 - val_acc: 0.9663\n",
      "Epoch 6/10\n",
      "33600/33600 [==============================] - 6s - loss: 0.0749 - acc: 0.9786 - val_loss: 0.0963 - val_acc: 0.9715\n",
      "Epoch 7/10\n",
      "33600/33600 [==============================] - 6s - loss: 0.0653 - acc: 0.9810 - val_loss: 0.0902 - val_acc: 0.9726\n",
      "Epoch 8/10\n",
      "33600/33600 [==============================] - 6s - loss: 0.0560 - acc: 0.9829 - val_loss: 0.0860 - val_acc: 0.9745\n",
      "Epoch 9/10\n",
      "33600/33600 [==============================] - 6s - loss: 0.0507 - acc: 0.9842 - val_loss: 0.0848 - val_acc: 0.9749\n",
      "Epoch 10/10\n",
      "33600/33600 [==============================] - 6s - loss: 0.0460 - acc: 0.9868 - val_loss: 0.0842 - val_acc: 0.9746\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f9d64e121d0>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model= Sequential()\n",
    "model.add(Flatten(input_shape=(28,28,1)))\n",
    "model.add(Dense(300, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "from keras.optimizers import Adadelta\n",
    "model.compile(optimizer=Adadelta(),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "\n",
    "model.fit(X_train,OHE_y_train,validation_split=0.2,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_19 (Flatten)         (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_38 (Dense)             (None, 200)               157000    \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_39 (Dense)             (None, 20)                4020      \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_40 (Dense)             (None, 10)                210       \n",
      "=================================================================\n",
      "Total params: 161,230\n",
      "Trainable params: 161,230\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 33600 samples, validate on 8400 samples\n",
      "Epoch 1/10\n",
      "33600/33600 [==============================] - 7s - loss: 0.5505 - acc: 0.8277 - val_loss: 0.2234 - val_acc: 0.9361\n",
      "Epoch 2/10\n",
      "33600/33600 [==============================] - 7s - loss: 0.2807 - acc: 0.9157 - val_loss: 0.1714 - val_acc: 0.9495\n",
      "Epoch 3/10\n",
      "33600/33600 [==============================] - 7s - loss: 0.2141 - acc: 0.9362 - val_loss: 0.1430 - val_acc: 0.9579\n",
      "Epoch 4/10\n",
      "33600/33600 [==============================] - 7s - loss: 0.1801 - acc: 0.9448 - val_loss: 0.1275 - val_acc: 0.9624\n",
      "Epoch 5/10\n",
      "33600/33600 [==============================] - 7s - loss: 0.1586 - acc: 0.9528 - val_loss: 0.1220 - val_acc: 0.9644\n",
      "Epoch 6/10\n",
      "33600/33600 [==============================] - 7s - loss: 0.1398 - acc: 0.9584 - val_loss: 0.1145 - val_acc: 0.9670\n",
      "Epoch 7/10\n",
      "33600/33600 [==============================] - 7s - loss: 0.1249 - acc: 0.9623 - val_loss: 0.1096 - val_acc: 0.9671\n",
      "Epoch 8/10\n",
      "33600/33600 [==============================] - 7s - loss: 0.1154 - acc: 0.9665 - val_loss: 0.1106 - val_acc: 0.9693\n",
      "Epoch 9/10\n",
      "33600/33600 [==============================] - 7s - loss: 0.1044 - acc: 0.9692 - val_loss: 0.1021 - val_acc: 0.9698\n",
      "Epoch 10/10\n",
      "33600/33600 [==============================] - 7s - loss: 0.0946 - acc: 0.9717 - val_loss: 0.1046 - val_acc: 0.9707\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f9d6475feb8>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model= Sequential()\n",
    "model.add(Flatten(input_shape=(28,28,1)))\n",
    "model.add(Dense(200, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(20, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "from keras.optimizers import Adadelta\n",
    "model.compile(optimizer=Adadelta(),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "\n",
    "model.fit(X_train,OHE_y_train,validation_split=0.2,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'PIL'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-b75b03d434de>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mPIL\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# img = cv2.imread('../input/scr_written_digits_1.png')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# img\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'PIL'"
     ]
    }
   ],
   "source": [
    "import PIL\n",
    "# img = cv2.imread('../input/scr_written_digits_1.png')\n",
    "# img"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
