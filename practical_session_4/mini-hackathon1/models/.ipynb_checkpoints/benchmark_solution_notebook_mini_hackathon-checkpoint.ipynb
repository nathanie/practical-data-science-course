{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense , Dropout , Lambda, Flatten, Conv2D, MaxPool2D, BatchNormalization, Input,Concatenate\n",
    "from keras.optimizers import SGD\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import model_from_json\n",
    "\n",
    "import cv2\n",
    "import glob\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "\n",
    "#set global parameters\n",
    "img_rows = 224\n",
    "img_cols = 224\n",
    "max_files = -1\n",
    "read_from_cache = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filelist = glob.glob('../input/train/*/*.*')\n",
    "categories = np.unique([x.split('/')[3] for x in filelist])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['classroompictures', 'diningpictures', 'entrancepictures',\n",
       "       'exhibitionpictures', 'stairspictures'],\n",
       "      dtype='<U18')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_image(path,img_rows,img_cols):\n",
    "    img = cv2.imread(path)\n",
    "    return cv2.resize(img, (img_cols, img_rows))\n",
    "\n",
    "def read_train(img_rows,img_cols,max_files):\n",
    "    \n",
    "    # img_rows & img_cols set the size of the image in the output\n",
    "    # max files is the maximal number of images to read from each category\n",
    "    # use max_files=-1 to read all images within the train subfolders\n",
    "    \n",
    "    X_train = []\n",
    "    y_train = []\n",
    "    \n",
    "    print('Read train images')\n",
    "    for j,category in enumerate(categories):\n",
    "        counter = 0\n",
    "        print('Load folder {}'.format(category))\n",
    "        path = os.path.join('..', 'input','train', category, '*.jpg')\n",
    "        files = glob.glob(path)\n",
    "        for fl in files:\n",
    "            flbase = os.path.basename(fl)\n",
    "            img = read_image(fl, img_rows, img_cols)\n",
    "            X_train.append(np.asarray(img))\n",
    "            y_train.append(j)\n",
    "            counter+=1\n",
    "            if (counter>=max_files)&(max_files>0):\n",
    "                break\n",
    "    \n",
    "    return np.array(X_train), np.array(y_train)\n",
    "\n",
    "def read_test(img_rows,img_cols):\n",
    "    X_test = []\n",
    "    ids = []\n",
    "    print('Read test images')\n",
    "    path = os.path.join('..', 'input','final_test', '*.jpg')\n",
    "    files = glob.glob(path)\n",
    "    for fl in files:\n",
    "        flbase = os.path.basename(fl)\n",
    "        img = read_image(fl, img_rows, img_cols)\n",
    "        X_test.append(np.asarray(img))\n",
    "        ids.append(fl.split('/')[-1])\n",
    "    \n",
    "    return np.array(ids), np.array(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cache_data(data, path):\n",
    "    # this is a helper function used to cache data once it was read and preprocessed\n",
    "    if os.path.isdir(os.path.dirname(path)):\n",
    "        file = open(path, 'wb')\n",
    "        pickle.dump(data, file)\n",
    "        file.close()\n",
    "    else:\n",
    "        print('Directory doesnt exists')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def restore_data(path):\n",
    "    # this is a helper function used to restore cached data\n",
    "    data = dict()\n",
    "    if os.path.isfile(path):\n",
    "        file = open(path, 'rb')\n",
    "        data = pickle.load(file)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_model(model,filename):\n",
    "    # this is a helper function used to save a keras NN model architecture and weights\n",
    "    json_string = model.to_json()\n",
    "    if not os.path.isdir('cache'):\n",
    "        os.mkdir('cache')\n",
    "    open(os.path.join('cache', filename+'_architecture.json'), 'w').write(json_string)\n",
    "    model.save_weights(os.path.join('cache', filename+'_model_weights.h5'), overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_model(filename):\n",
    "    # this is a helper function used to restore a keras NN model architecture and weights\n",
    "    model = model_from_json(open(os.path.join('cache', filename+'_architecture.json')).read())\n",
    "    model.load_weights(os.path.join('cache', filename+'_model_weights.h5'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read train images\n",
      "Load folder classroompictures\n",
      "Load folder diningpictures\n",
      "Load folder entrancepictures\n",
      "Load folder exhibitionpictures\n",
      "Load folder stairspictures\n"
     ]
    }
   ],
   "source": [
    "# small sized pics - convolutional fresh model\n",
    "#set global parameters\n",
    "img_rows = 32\n",
    "img_cols = 32\n",
    "if not read_from_cache:\n",
    "    X_train, y_train = read_train(img_rows,img_cols,max_files)\n",
    "    cache_data(X_train,'../processed_input/X_train_{}X{}X3_{}_max_samples'.format(img_rows,img_cols,max_files))\n",
    "    cache_data(y_train,'../processed_input/y_train_{}_max_samples'.format(max_files))\n",
    "else:\n",
    "    X_train = restore_data('../processed_input/X_train_{}X{}X3_{}_max_samples'.format(img_rows,img_cols,max_files))\n",
    "    y_train = restore_data('../processed_input/y_train_{}_max_samples'.format(max_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils.np_utils import to_categorical\n",
    "OHE_y_train = to_categorical(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 31, 31, 48)        624       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 30, 30, 32)        6176      \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 29, 29, 16)        2064      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 14, 14, 16)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 3136)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 5)                 15685     \n",
      "=================================================================\n",
      "Total params: 24,549\n",
      "Trainable params: 24,549\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model= Sequential()\n",
    "model.add(Conv2D(48,(2,2),activation='relu',input_shape=(img_rows,img_cols,3)))\n",
    "model.add(Conv2D(32,(2,2),activation='relu'))\n",
    "model.add(Conv2D(16,(2,2),activation='relu'))\n",
    "model.add(MaxPool2D(pool_size=(2,2),padding='valid'))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(5, activation='softmax'))\n",
    "model.summary()\n",
    "from keras.optimizers import Adadelta\n",
    "model.compile(optimizer=Adadelta(),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "#model.fit(X_train,OHE_y_train,validation_split=0.2,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "5000/5000 [==============================] - 54s - loss: 0.3721 - acc: 0.8756 - val_loss: 0.1203 - val_acc: 0.9586\n",
      "Epoch 2/4\n",
      "5000/5000 [==============================] - 53s - loss: 0.0934 - acc: 0.9693 - val_loss: 0.0625 - val_acc: 0.9790\n",
      "Epoch 3/4\n",
      "5000/5000 [==============================] - 53s - loss: 0.0634 - acc: 0.9803 - val_loss: 0.0313 - val_acc: 0.9910\n",
      "Epoch 4/4\n",
      "5000/5000 [==============================] - 53s - loss: 0.0472 - acc: 0.9846 - val_loss: 0.0321 - val_acc: 0.9898\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f1d68121f98>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "datagen = ImageDataGenerator(\n",
    "    horizontal_flip=True,\n",
    "    rescale=0.5,\n",
    "    shear_range=0.3,\n",
    "    zoom_range=0.4,\n",
    "    rotation_range=25,\n",
    "    width_shift_range=0.5,\n",
    "    height_shift_range=0.5\n",
    "    )\n",
    "\n",
    "model.fit_generator(datagen.flow(X_train, OHE_y_train, batch_size=16),\n",
    "                    validation_data=datagen.flow(X_train,OHE_y_train),\n",
    "                    steps_per_epoch=5000,validation_steps = 1000, epochs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read test images\n"
     ]
    }
   ],
   "source": [
    "ids, X_test = read_test(img_rows=img_rows,img_cols=img_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pred = model.predict(X_test)\n",
    "subm = pd.DataFrame(np.round(pred,decimals=5))\n",
    "subm.columns = ['classroompictures', 'diningpictures', 'entrancepictures', 'exhibitionpictures', 'stairspictures']\n",
    "subm['Id'] = ids\n",
    "subm['index'] = subm.Id.apply(lambda x: int(x.split('.')[0]))\n",
    "subm.sort_values(by=['index'],inplace=True,ascending=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_labels = pd.read_csv('../test_labels/test_labels.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.133201424766167"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "log_loss(y_pred=subm.loc[:,'classroompictures':'stairspictures'],\n",
    "         y_true=test_labels.loc[:,'classroompictures':'stairspictures'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### the result doesn't seem so good... \n",
    "##### but we have to take into our considiration the sensetivity of the target function to mistakes with high probability,\n",
    "\n",
    "##### once we remove these mistakes we get much better results..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.33696196734523159"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_loss(y_pred=np.clip(subm.loc[:,'classroompictures':'stairspictures'],a_max=0.99,a_min=0.01),\n",
    "         y_true=test_labels.loc[:,'classroompictures':'stairspictures'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    precision    recall  f1-score   support\n",
      "\n",
      " classroompictures       0.94      1.00      0.97        16\n",
      "    diningpictures       0.83      1.00      0.91        15\n",
      "  entrancepictures       0.93      0.93      0.93        15\n",
      "exhibitionpictures       1.00      0.88      0.94        17\n",
      "    stairspictures       1.00      0.87      0.93        15\n",
      "\n",
      "       avg / total       0.94      0.94      0.94        78\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "class_pred = subm.loc[:,'classroompictures':'stairspictures'].idxmax(axis = 1)\n",
    "class_labels = test_labels.loc[:,'classroompictures':'stairspictures'].idxmax(axis = 1)\n",
    "print(classification_report(y_pred=class_pred,\n",
    "                            y_true=class_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[16  0  0  0  0]\n",
      " [ 0 15  0  0  0]\n",
      " [ 0  1 14  0  0]\n",
      " [ 1  0  1 15  0]\n",
      " [ 0  2  0  0 13]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_pred=class_pred,y_true=class_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fname = 'input_{}X{}'.format(img_rows,img_cols)+'model6'\n",
    "save_model(model,fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1.00000000e+00,   7.21504562e-32,   0.00000000e+00,\n",
       "          8.56971976e-23,   0.00000000e+00],\n",
       "       [  8.62869763e-16,   4.57413600e-27,   0.00000000e+00,\n",
       "          1.00000000e+00,   0.00000000e+00],\n",
       "       [  0.00000000e+00,   9.63868203e-08,   0.00000000e+00,\n",
       "          9.99999940e-01,   4.19215097e-12],\n",
       "       [  9.61254774e-12,   1.66666508e-01,   2.19697085e-06,\n",
       "          1.78561067e-14,   8.33331287e-01],\n",
       "       [  4.60548041e-24,   2.06996503e-07,   9.99999821e-01,\n",
       "          2.78877602e-23,   2.28095098e-10],\n",
       "       [  1.00000000e+00,   2.23427185e-33,   0.00000000e+00,\n",
       "          3.61441083e-27,   0.00000000e+00],\n",
       "       [  2.65998505e-21,   1.00000000e+00,   1.21811217e-33,\n",
       "          0.00000000e+00,   0.00000000e+00],\n",
       "       [  9.66390118e-30,   6.06939992e-08,   9.99999940e-01,\n",
       "          2.78209189e-33,   1.11155872e-19],\n",
       "       [  5.66703692e-27,   1.29303618e-13,   1.00000000e+00,\n",
       "          1.62625858e-38,   3.60824956e-14],\n",
       "       [  1.89490991e-22,   8.37892234e-01,   1.62080079e-01,\n",
       "          1.47347233e-19,   2.76745996e-05],\n",
       "       [  2.93350671e-30,   7.25755456e-10,   1.88149923e-37,\n",
       "          1.00000000e+00,   3.07121726e-23],\n",
       "       [  4.34438000e-04,   1.66667625e-01,   4.99469161e-01,\n",
       "          3.33428711e-01,   2.82596496e-17],\n",
       "       [  1.83425136e-25,   1.24845465e-19,   3.71500292e-27,\n",
       "          1.16404226e-04,   9.99883652e-01],\n",
       "       [  7.41576151e-26,   1.14650052e-11,   4.75926369e-01,\n",
       "          1.67424906e-28,   5.24073660e-01],\n",
       "       [  1.00000000e+00,   3.11122314e-33,   0.00000000e+00,\n",
       "          5.75898596e-33,   0.00000000e+00],\n",
       "       [  1.26277683e-20,   0.00000000e+00,   0.00000000e+00,\n",
       "          1.00000000e+00,   0.00000000e+00],\n",
       "       [  1.62023905e-24,   3.81647000e-15,   3.72000041e-39,\n",
       "          1.00000000e+00,   2.55024303e-34],\n",
       "       [  4.02155357e-22,   5.02042203e-05,   9.99949753e-01,\n",
       "          7.18633179e-27,   6.47624399e-09],\n",
       "       [  0.00000000e+00,   1.83206895e-15,   0.00000000e+00,\n",
       "          1.00000000e+00,   2.86394806e-36],\n",
       "       [  3.05509451e-10,   7.40456879e-02,   1.97524484e-12,\n",
       "          9.25953925e-01,   3.70327598e-07],\n",
       "       [  2.18566206e-28,   4.42023110e-03,   5.87969173e-10,\n",
       "          6.78597504e-29,   9.95579720e-01],\n",
       "       [  9.76943195e-01,   1.57241994e-14,   6.16108445e-22,\n",
       "          2.30567958e-02,   0.00000000e+00],\n",
       "       [  1.73033669e-14,   3.76047492e-02,   2.22967124e-27,\n",
       "          9.62395251e-01,   2.80738186e-27],\n",
       "       [  1.00000000e+00,   1.86070098e-21,   2.33958124e-37,\n",
       "          1.89918435e-18,   0.00000000e+00],\n",
       "       [  2.69316624e-17,   1.66728973e-01,   8.32705498e-01,\n",
       "          6.75060991e-15,   5.65555529e-04],\n",
       "       [  2.68680759e-24,   1.38628366e-06,   9.99746621e-01,\n",
       "          7.89668304e-24,   2.52005993e-04],\n",
       "       [  1.09888170e-27,   7.72410488e-07,   9.99999285e-01,\n",
       "          3.18189289e-25,   1.72606418e-09],\n",
       "       [  9.62848573e-20,   8.33743393e-13,   2.87750829e-19,\n",
       "          5.03631952e-07,   9.99999523e-01],\n",
       "       [  9.15812634e-05,   8.33139122e-01,   1.66766867e-01,\n",
       "          4.86450102e-10,   2.41288808e-06],\n",
       "       [  7.39930652e-27,   8.33365440e-01,   5.65982140e-23,\n",
       "          1.66627839e-01,   6.71457337e-06],\n",
       "       [  0.00000000e+00,   1.00000000e+00,   0.00000000e+00,\n",
       "          1.87964394e-34,   0.00000000e+00],\n",
       "       [  9.49477198e-18,   1.86937072e-07,   1.43229503e-19,\n",
       "          7.70076092e-09,   9.99999821e-01],\n",
       "       [  8.07058555e-27,   3.64430464e-13,   8.75376216e-10,\n",
       "          1.70730575e-17,   1.00000000e+00],\n",
       "       [  1.60663615e-25,   3.29182406e-08,   1.00000000e+00,\n",
       "          0.00000000e+00,   3.62847824e-21],\n",
       "       [  1.00000000e+00,   4.97431886e-18,   5.02661899e-27,\n",
       "          5.38009959e-15,   0.00000000e+00],\n",
       "       [  0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          1.00000000e+00,   0.00000000e+00],\n",
       "       [  2.22508570e-29,   3.55556068e-07,   9.99999702e-01,\n",
       "          1.10893451e-36,   3.17497552e-36],\n",
       "       [  6.02974639e-33,   9.99914706e-01,   8.52682570e-05,\n",
       "          3.34768648e-27,   5.00818727e-11],\n",
       "       [  4.53261503e-22,   3.60028167e-07,   9.99999583e-01,\n",
       "          1.86995241e-26,   3.08940509e-13],\n",
       "       [  1.00000000e+00,   4.03401819e-28,   0.00000000e+00,\n",
       "          6.29387434e-17,   0.00000000e+00],\n",
       "       [  1.00000000e+00,   2.18569069e-39,   0.00000000e+00,\n",
       "          3.92405945e-22,   0.00000000e+00],\n",
       "       [  0.00000000e+00,   2.89173315e-21,   0.00000000e+00,\n",
       "          1.00000000e+00,   1.15568509e-34],\n",
       "       [  2.28719913e-37,   0.00000000e+00,   0.00000000e+00,\n",
       "          1.00000000e+00,   0.00000000e+00],\n",
       "       [  5.90978190e-27,   6.18216589e-12,   2.95930622e-08,\n",
       "          7.23119525e-24,   1.00000000e+00],\n",
       "       [  1.00000000e+00,   9.47630131e-31,   0.00000000e+00,\n",
       "          1.82915220e-21,   0.00000000e+00],\n",
       "       [  5.17225615e-18,   1.74007238e-15,   2.54598519e-21,\n",
       "          3.90362222e-12,   1.00000000e+00],\n",
       "       [  8.27257097e-01,   2.88599450e-03,   1.59539399e-04,\n",
       "          1.69491649e-01,   2.05756558e-04],\n",
       "       [  4.98369545e-01,   5.01630485e-01,   3.31282288e-16,\n",
       "          2.65103363e-18,   0.00000000e+00],\n",
       "       [  1.68836856e-32,   7.88007021e-01,   2.11992919e-01,\n",
       "          1.02836269e-28,   8.51487982e-08],\n",
       "       [  0.00000000e+00,   1.00000000e+00,   0.00000000e+00,\n",
       "          1.62438216e-28,   0.00000000e+00],\n",
       "       [  1.00000000e+00,   7.41973725e-25,   0.00000000e+00,\n",
       "          7.10094037e-20,   0.00000000e+00],\n",
       "       [  1.00000000e+00,   8.75354917e-21,   0.00000000e+00,\n",
       "          5.75931455e-15,   0.00000000e+00],\n",
       "       [  7.65884867e-09,   3.15907644e-04,   4.06338695e-05,\n",
       "          1.16382949e-02,   9.88005161e-01],\n",
       "       [  1.58920818e-28,   1.69231010e-15,   5.05070712e-15,\n",
       "          1.35697669e-16,   1.00000000e+00],\n",
       "       [  1.69214022e-18,   7.08070928e-12,   9.99999940e-01,\n",
       "          5.68557148e-08,   1.34208067e-09],\n",
       "       [  9.94874959e-14,   1.65670902e-01,   2.41906106e-04,\n",
       "          1.97051129e-13,   8.34087193e-01],\n",
       "       [  1.60747729e-16,   0.00000000e+00,   0.00000000e+00,\n",
       "          1.00000000e+00,   0.00000000e+00],\n",
       "       [  3.68846030e-32,   1.81571081e-16,   1.24462473e-22,\n",
       "          1.55977791e-12,   1.00000000e+00],\n",
       "       [  1.40074118e-23,   6.72184299e-08,   9.99999940e-01,\n",
       "          5.90467367e-25,   2.57684681e-15],\n",
       "       [  5.59396690e-24,   3.97021154e-07,   2.07061821e-04,\n",
       "          1.00569852e-15,   9.99792576e-01],\n",
       "       [  1.79745742e-36,   1.00000000e+00,   1.06461114e-32,\n",
       "          2.90189574e-39,   0.00000000e+00],\n",
       "       [  1.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00],\n",
       "       [  2.57339637e-19,   1.00000000e+00,   4.99954124e-15,\n",
       "          3.13549159e-34,   1.21553971e-12],\n",
       "       [  1.15239925e-06,   9.99997914e-01,   5.89590492e-08,\n",
       "          1.68293290e-09,   8.85953568e-07],\n",
       "       [  4.56250787e-01,   1.94878460e-08,   2.49135312e-27,\n",
       "          5.43749154e-01,   0.00000000e+00],\n",
       "       [  1.00000000e+00,   8.06279531e-39,   0.00000000e+00,\n",
       "          1.07057902e-29,   0.00000000e+00],\n",
       "       [  6.67815059e-02,   6.95716075e-24,   0.00000000e+00,\n",
       "          9.33218479e-01,   0.00000000e+00],\n",
       "       [  1.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00],\n",
       "       [  3.90821215e-30,   5.76871003e-07,   9.99999464e-01,\n",
       "          1.24271544e-32,   6.78513495e-12],\n",
       "       [  6.91682178e-37,   1.41018620e-12,   1.00000000e+00,\n",
       "          0.00000000e+00,   6.99833048e-29],\n",
       "       [  0.00000000e+00,   3.59629774e-12,   0.00000000e+00,\n",
       "          1.00000000e+00,   1.06279908e-10],\n",
       "       [  1.23036361e-28,   5.29570752e-06,   9.99994695e-01,\n",
       "          8.52659357e-28,   2.94336299e-16],\n",
       "       [  2.38217578e-25,   9.54028234e-09,   1.00000000e+00,\n",
       "          5.99294269e-29,   6.43104625e-10],\n",
       "       [  6.46740924e-23,   5.18152627e-15,   1.21942205e-17,\n",
       "          5.82723317e-25,   1.00000000e+00],\n",
       "       [  4.26376327e-30,   1.00000000e+00,   2.44166182e-27,\n",
       "          7.13600956e-28,   8.38460090e-31],\n",
       "       [  1.84847692e-35,   1.00000000e+00,   1.06882581e-16,\n",
       "          0.00000000e+00,   9.81968167e-39],\n",
       "       [  3.33272904e-01,   6.49846554e-01,   1.68790556e-02,\n",
       "          1.47136052e-06,   9.73198062e-37],\n",
       "       [  0.00000000e+00,   1.00000000e+00,   2.10089929e-25,\n",
       "          0.00000000e+00,   0.00000000e+00]], dtype=float32)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1 = read_model('input_32X32model1')\n",
    "model2 = read_model('input_32X32model2')\n",
    "model3 = read_model('input_32X32model3')\n",
    "model4 = read_model('input_32X32model4')\n",
    "model5 = read_model('input_32X32model5')\n",
    "model6 = read_model('input_32X32model6')\n",
    "\n",
    "m1_pred = model1.predict(X_test)\n",
    "m2_pred = model2.predict(X_test)\n",
    "m3_pred = model3.predict(X_test)\n",
    "m4_pred = model4.predict(X_test)\n",
    "m5_pred = model5.predict(X_test)\n",
    "m6_pred = model6.predict(X_test)\n",
    "\n",
    "prediction = m1_pred + m2_pred + m3_pred + m4_pred + m5_pred + m6_pred \n",
    "prediction /=6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.162778182242\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      " classroompictures       1.00      0.94      0.97        16\n",
      "    diningpictures       0.94      1.00      0.97        15\n",
      "  entrancepictures       0.88      1.00      0.94        15\n",
      "exhibitionpictures       1.00      0.88      0.94        17\n",
      "    stairspictures       1.00      1.00      1.00        15\n",
      "\n",
      "       avg / total       0.97      0.96      0.96        78\n",
      "\n",
      "[[15  1  0  0  0]\n",
      " [ 0 15  0  0  0]\n",
      " [ 0  0 15  0  0]\n",
      " [ 0  0  2 15  0]\n",
      " [ 0  0  0  0 15]]\n"
     ]
    }
   ],
   "source": [
    "subm = pd.DataFrame(np.round(prediction,decimals=5))\n",
    "subm.columns = ['classroompictures', 'diningpictures', 'entrancepictures', 'exhibitionpictures', 'stairspictures']\n",
    "subm['Id'] = ids\n",
    "subm['index'] = subm.Id.apply(lambda x: int(x.split('.')[0]))\n",
    "subm.sort_values(by=['index'],inplace=True,ascending=True)\n",
    "print(log_loss(y_pred=np.clip(subm.loc[:,'classroompictures':'stairspictures'],a_max=0.99,a_min=0.01),\n",
    "         y_true=test_labels.loc[:,'classroompictures':'stairspictures']))\n",
    "class_pred = subm.loc[:,'classroompictures':'stairspictures'].idxmax(axis = 1)\n",
    "class_labels = test_labels.loc[:,'classroompictures':'stairspictures'].idxmax(axis = 1)\n",
    "print(classification_report(y_pred=class_pred,\n",
    "                            y_true=class_labels))\n",
    "print(confusion_matrix(y_pred=class_pred,y_true=class_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### lets try pseudo labeling \n",
    "\n",
    "this means we're going to take our best prediction for the test set, and use it as the ground true for test set, then retrain with both train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_new = np.concatenate((X_train,X_test))\n",
    "OHE_y_train_new = np.concatenate((OHE_y_train,np.round(prediction,0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### lets compare our results with uniform prediction scoring:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.6094379124341003"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = subm.loc[:,'classroompictures':'stairspictures']\n",
    "for i in temp.columns:\n",
    "    temp[i]=0.2\n",
    "log_loss(y_pred=temp,y_true=test_labels.loc[:,'classroompictures':'stairspictures'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### the above calculation means that any result with log_loss above 1.6 - is practically not learning anything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classroompictures</th>\n",
       "      <th>diningpictures</th>\n",
       "      <th>entrancepictures</th>\n",
       "      <th>exhibitionpictures</th>\n",
       "      <th>stairspictures</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    classroompictures  diningpictures  entrancepictures  exhibitionpictures  \\\n",
       "65                0.2             0.2               0.2                 0.2   \n",
       "8                 0.2             0.2               0.2                 0.2   \n",
       "4                 0.2             0.2               0.2                 0.2   \n",
       "7                 0.2             0.2               0.2                 0.2   \n",
       "69                0.2             0.2               0.2                 0.2   \n",
       "\n",
       "    stairspictures  \n",
       "65             0.2  \n",
       "8              0.2  \n",
       "4              0.2  \n",
       "7              0.2  \n",
       "69             0.2  "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read train images\n",
      "Load folder classroompictures\n",
      "Load folder diningpictures\n",
      "Load folder entrancepictures\n",
      "Load folder exhibitionpictures\n",
      "Load folder stairspictures\n"
     ]
    }
   ],
   "source": [
    "# medium sized pics - convolutional pretrained model (retraining the last added layer)\n",
    "#set global parameters\n",
    "img_rows = 224\n",
    "img_cols = 224\n",
    "if not read_from_cache:\n",
    "    X_train, y_train = read_train(img_rows,img_cols,max_files)\n",
    "    cache_data(X_train,'../processed_input/X_train_{}X{}X3_{}_max_samples'.format(img_rows,img_cols,max_files))\n",
    "    cache_data(y_train,'../processed_input/y_train_{}_max_samples'.format(max_files))\n",
    "else:\n",
    "    X_train = restore_data('../processed_input/X_train_{}X{}X3_{}_max_samples'.format(img_rows,img_cols,max_files))\n",
    "    y_train = restore_data('../processed_input/y_train_{}_max_samples'.format(max_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_16 (InputLayer)        (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.layers import Input, Flatten, Dense\n",
    "from keras.models import Model\n",
    "\n",
    "vgg16 = VGG16(weights='imagenet', include_top=False,input_shape=(img_rows,img_cols,3))\n",
    "vgg16.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_17 (InputLayer)        (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 3, 3, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 4608)              0         \n",
      "_________________________________________________________________\n",
      "predictions (Dense)          (None, 5)                 23045     \n",
      "=================================================================\n",
      "Total params: 14,737,733\n",
      "Trainable params: 23,045\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#model = create_model(img_rows, img_cols, color_type_global)\n",
    "# Generate a model with all layers (with top)\n",
    "vgg16 = VGG16(weights='imagenet', include_top=True)\n",
    "\n",
    "for l in vgg16.layers:\n",
    "    l.trainable = False\n",
    "\n",
    "x = MaxPool2D(pool_size=(2,2),padding='valid')(vgg16.layers[-5].output)\n",
    "x = Flatten()(x)\n",
    "x = Dense(5,activation='softmax',name = 'predictions')(x)\n",
    "model = Model(inputs = vgg16.input,outputs = x)\n",
    "\n",
    "from keras.optimizers import adadelta, adam, SGD\n",
    "#sgd = SGD(lr=0.001, decay=1e-5, momentum=0.95, nesterov=True)\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "    \n",
    "model.summary()   \n",
    "    \n",
    "# vgg16.layers['predictions'].trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 141s - loss: 0.4677 - acc: 0.9328 - val_loss: 0.1327 - val_acc: 0.9784\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f1d08add400>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(datagen.flow(X_train, OHE_y_train, batch_size=16),\n",
    "                    validation_data=datagen.flow(X_train,OHE_y_train),\n",
    "                    steps_per_epoch=1000,validation_steps = 100, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#model.fit(X_train,OHE_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read test images\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.99433041354466645"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids, X_test = read_test(img_rows=img_rows,img_cols=img_cols)\n",
    "pred = model.predict(X_test)\n",
    "subm = pd.DataFrame(np.round(pred,decimals=5))\n",
    "subm.columns = ['classroompictures', 'diningpictures', 'entrancepictures', 'exhibitionpictures', 'stairspictures']\n",
    "subm['Id'] = ids\n",
    "subm['index'] = subm.Id.apply(lambda x: int(x.split('.')[0]))\n",
    "subm.sort_values(by=['index'],inplace=True,ascending=True)\n",
    "log_loss(y_pred=subm.loc[:,'classroompictures':'stairspictures'],\n",
    "         y_true=test_labels.loc[:,'classroompictures':'stairspictures'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.219456771770731"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_loss(y_pred=np.clip(subm.loc[:,'classroompictures':'stairspictures'],a_max=0.99,a_min=0.01),\n",
    "         y_true=test_labels.loc[:,'classroompictures':'stairspictures'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    precision    recall  f1-score   support\n",
      "\n",
      " classroompictures       1.00      1.00      1.00        16\n",
      "    diningpictures       1.00      0.93      0.97        15\n",
      "  entrancepictures       0.83      1.00      0.91        15\n",
      "exhibitionpictures       1.00      1.00      1.00        17\n",
      "    stairspictures       1.00      0.87      0.93        15\n",
      "\n",
      "       avg / total       0.97      0.96      0.96        78\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "class_pred = subm.loc[:,'classroompictures':'stairspictures'].idxmax(axis = 1)\n",
    "class_labels = test_labels.loc[:,'classroompictures':'stairspictures'].idxmax(axis = 1)\n",
    "print(classification_report(y_pred=class_pred,\n",
    "                            y_true=class_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[16  0  0  0  0]\n",
      " [ 0 14  1  0  0]\n",
      " [ 0  0 15  0  0]\n",
      " [ 0  0  0 17  0]\n",
      " [ 0  0  2  0 13]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(confusion_matrix(y_pred=class_pred,y_true=class_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_model(model,'vgg_based_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
